{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-10-07T15:31:15.097249Z",
     "start_time": "2025-10-07T15:31:15.087407Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load CSV\n",
    "df = pd.read_csv('../dat/porto.csv')\n",
    "\n",
    "# Create a summary DataFrame\n",
    "summary = pd.DataFrame(columns=['Column', 'Nulls', 'Unique Values', 'Repeated Values'])\n"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-07T15:31:15.132136Z",
     "start_time": "2025-10-07T15:31:15.123593Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for col in df.columns:\n",
    "    nulls = df[col].isnull().sum()\n",
    "    unique_vals = df[col].nunique(dropna=True)\n",
    "    repeated = len(df[col]) - unique_vals - nulls  # Total minus unique values and nulls\n",
    "    summary = pd.concat([summary,\n",
    "                         pd.DataFrame([[col, nulls, unique_vals, repeated]],\n",
    "                                             columns=['Column', 'Nulls', 'Unique Values', 'Repeated Values'])],\n",
    "                        ignore_index=True)\n",
    "\n",
    "# Show summary\n",
    "print(summary)"
   ],
   "id": "119a29965f17440a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                       Column Nulls Unique Values  \\\n",
      "0  version https://git-lfs.github.com/spec/v1     0             2   \n",
      "\n",
      "  Repeated Values  \n",
      "0               0  \n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Important info from the output:\n",
    "\n",
    "    - TRIP_ID has repeated values, this must change so that it can be used as a PRIMARY KEY\n",
    "\n",
    "    - DAY_TYPE has 1 unique value so it doesn't provide additional info, therefore it can be eliminated\n",
    "\n",
    "    - The number of taxi drivers is 448 as TAXI_ID has that number of unique values\n",
    "\n",
    "    - CALL_TYPE doesn't have to be modified as it matches the info given in the assignment"
   ],
   "id": "f376ab880af333b4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "To solve the TRIP_ID duplicates problem we will keep the row that has the earliest datetime compared to the other duplicates",
   "id": "8e5d9906ea90a641"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-07T15:34:51.827055Z",
     "start_time": "2025-10-07T15:34:51.822355Z"
    }
   },
   "cell_type": "code",
   "source": "print(df.columns.tolist())\n",
   "id": "def5d2e280cbf5b5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['version https://git-lfs.github.com/spec/v1']\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-07T15:34:11.752053Z",
     "start_time": "2025-10-07T15:34:11.728260Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df.sort_values(by=['TRIP_ID', 'TIMESTAMP'], ascending=[True, True], inplace=True)\n",
    "\n",
    "df.drop_duplicates(subset='TRIP_ID', keep='first', inplace=True)\n",
    "\n",
    "print(\"Number of duplicated TRIP_ID after deduplication:\", df['TRIP_ID'].duplicated().sum())"
   ],
   "id": "b62eab5d17b6c358",
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'TRIP_ID'",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyError\u001B[39m                                  Traceback (most recent call last)",
      "\u001B[32m~\\AppData\\Local\\Temp\\ipykernel_27036\\1768853808.py\u001B[39m in \u001B[36m?\u001B[39m\u001B[34m()\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m1\u001B[39m df.sort_values(by=[\u001B[33m'TRIP_ID'\u001B[39m, \u001B[33m'TIMESTAMP'\u001B[39m], ascending=[\u001B[38;5;28;01mTrue\u001B[39;00m, \u001B[38;5;28;01mTrue\u001B[39;00m], inplace=\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[32m      2\u001B[39m \n\u001B[32m      3\u001B[39m df.drop_duplicates(subset=\u001B[33m'TRIP_ID'\u001B[39m, keep=\u001B[33m'first'\u001B[39m, inplace=\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[32m      4\u001B[39m \n",
      "\u001B[32m~\\vlddatavgit\\.venv\\Lib\\site-packages\\pandas\\core\\frame.py\u001B[39m in \u001B[36m?\u001B[39m\u001B[34m(self, by, axis, ascending, inplace, kind, na_position, ignore_index, key)\u001B[39m\n\u001B[32m   7190\u001B[39m                 f\"Length of ascending ({len(ascending)})\"  \u001B[38;5;66;03m# type: ignore[arg-type]\u001B[39;00m\n\u001B[32m   7191\u001B[39m                 f\" != length of by ({len(by)})\"\n\u001B[32m   7192\u001B[39m             )\n\u001B[32m   7193\u001B[39m         \u001B[38;5;28;01mif\u001B[39;00m len(by) > \u001B[32m1\u001B[39m:\n\u001B[32m-> \u001B[39m\u001B[32m7194\u001B[39m             keys = [self._get_label_or_level_values(x, axis=axis) \u001B[38;5;28;01mfor\u001B[39;00m x \u001B[38;5;28;01min\u001B[39;00m by]\n\u001B[32m   7195\u001B[39m \n\u001B[32m   7196\u001B[39m             \u001B[38;5;66;03m# need to rewrap columns in Series to apply key function\u001B[39;00m\n\u001B[32m   7197\u001B[39m             \u001B[38;5;28;01mif\u001B[39;00m key \u001B[38;5;28;01mis\u001B[39;00m \u001B[38;5;28;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "\u001B[32m~\\vlddatavgit\\.venv\\Lib\\site-packages\\pandas\\core\\generic.py\u001B[39m in \u001B[36m?\u001B[39m\u001B[34m(self, key, axis)\u001B[39m\n\u001B[32m   1910\u001B[39m             values = self.xs(key, axis=other_axes[\u001B[32m0\u001B[39m])._values\n\u001B[32m   1911\u001B[39m         \u001B[38;5;28;01melif\u001B[39;00m self._is_level_reference(key, axis=axis):\n\u001B[32m   1912\u001B[39m             values = self.axes[axis].get_level_values(key)._values\n\u001B[32m   1913\u001B[39m         \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1914\u001B[39m             \u001B[38;5;28;01mraise\u001B[39;00m KeyError(key)\n\u001B[32m   1915\u001B[39m \n\u001B[32m   1916\u001B[39m         \u001B[38;5;66;03m# Check for duplicates\u001B[39;00m\n\u001B[32m   1917\u001B[39m         \u001B[38;5;28;01mif\u001B[39;00m values.ndim > \u001B[32m1\u001B[39m:\n",
      "\u001B[31mKeyError\u001B[39m: 'TRIP_ID'"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Eliminate the column DAY_TYPE",
   "id": "fcee2f94a27e411d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-07T15:31:15.279763900Z",
     "start_time": "2025-10-07T15:11:15.670505Z"
    }
   },
   "cell_type": "code",
   "source": "df.drop(columns=['DAY_TYPE'], inplace=True)",
   "id": "7ac29d50aab05513",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "CHECK DATATYPES",
   "id": "26cf626021128b51"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-07T15:31:15.283876900Z",
     "start_time": "2025-10-07T15:11:19.968286Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df['TIMESTAMP'] = pd.to_datetime(df['TIMESTAMP'], errors='coerce')\n",
    "\n",
    "expected_dtypes = {\n",
    "    'TRIP_ID': 'int64',\n",
    "    'CALL_TYPE': 'object',\n",
    "    'ORIGIN_CALL': 'float64',  # could be int but NaNs make it float\n",
    "    'ORIGIN_STAND': 'float64', # same as above\n",
    "    'TAXI_ID': 'int64',\n",
    "    'TIMESTAMP': 'datetime64[ns]',\n",
    "    'MISSING_DATA': 'bool',\n",
    "    'POLYLINE': 'object'\n",
    "}\n",
    "\n",
    "dtype_summary = pd.DataFrame(columns=['Column', 'Expected', 'Actual', 'Match'])\n",
    "for col in df.columns:\n",
    "    actual = str(df[col].dtype)\n",
    "    expected = expected_dtypes.get(col, 'Unknown')\n",
    "    match = actual == expected\n",
    "    dtype_summary = pd.concat([dtype_summary,\n",
    "                               pd.DataFrame([[col, expected, actual, match]],\n",
    "                                            columns=['Column','Expected','Actual','Match'])],\n",
    "                              ignore_index=True)\n",
    "\n",
    "print(\"Datatype check per column:\\n\", dtype_summary)"
   ],
   "id": "f171db1bb5ba6807",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datatype check per column:\n",
      "          Column        Expected          Actual Match\n",
      "0       TRIP_ID           int64           int64  True\n",
      "1     CALL_TYPE          object          object  True\n",
      "2   ORIGIN_CALL         float64         float64  True\n",
      "3  ORIGIN_STAND         float64         float64  True\n",
      "4       TAXI_ID           int64           int64  True\n",
      "5     TIMESTAMP  datetime64[ns]  datetime64[ns]  True\n",
      "6  MISSING_DATA            bool            bool  True\n",
      "7      POLYLINE          object          object  True\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Check if the relationships between the values from CALL_TYPE, ORIGIN_CALL and ORIGIN_STAND match the rules specified",
   "id": "3dd6bc8269f22db1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-07T15:31:15.283876900Z",
     "start_time": "2025-10-07T15:11:25.795716Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Function to check if a value is a valid numeric ID\n",
    "def is_valid_id(val):\n",
    "    return pd.notnull(val) and isinstance(val, (int, float))\n",
    "\n",
    "# Rule 1: CALL_TYPE 'A', ORIGIN_CALL numeric, ORIGIN_STAND null\n",
    "rule_A = df[df['CALL_TYPE']=='A']\n",
    "rule_A_check = rule_A['ORIGIN_CALL'].apply(lambda x: is_valid_id(x)) & rule_A['ORIGIN_STAND'].isnull()\n",
    "print(\"CALL_TYPE 'A' compliance with datatype:\", rule_A_check.mean())\n",
    "\n",
    "# Rule 2: CALL_TYPE 'B', ORIGIN_CALL null, ORIGIN_STAND numeric\n",
    "rule_B = df[df['CALL_TYPE']=='B']\n",
    "rule_B_check = rule_B['ORIGIN_STAND'].apply(lambda x: is_valid_id(x)) & rule_B['ORIGIN_CALL'].isnull()\n",
    "print(\"CALL_TYPE 'B' compliance with datatype:\", rule_B_check.mean())\n",
    "\n",
    "# Rule 3: CALL_TYPE 'C', ORIGIN_CALL null, ORIGIN_STAND null\n",
    "rule_C = df[df['CALL_TYPE']=='C']\n",
    "rule_C_check = rule_C['ORIGIN_CALL'].isnull() & rule_C['ORIGIN_STAND'].isnull()\n",
    "print(\"CALL_TYPE 'C' compliance with datatype:\", rule_C_check.mean())"
   ],
   "id": "54d4ff7c0284c3ba",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CALL_TYPE 'A' compliance with datatype: 1.0\n",
      "CALL_TYPE 'B' compliance with datatype: 0.9861813637925322\n",
      "CALL_TYPE 'C' compliance with datatype: 1.0\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "- 100% of the rows where CALL_TYPE='A' satisfy the rule\n",
    "- 98.6% of the rows where CALL_TYPE='B' violate the rule\n",
    "- 100% of the rows where CALL_TYPE='C' satisfy the rule\n",
    "\n",
    "Filter the rows that violate the rule"
   ],
   "id": "117ac0b1ca2c08aa"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-07T15:31:15.283876900Z",
     "start_time": "2025-10-07T15:11:35.297526Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Filter rows where CALL_TYPE is 'B'\n",
    "rule_B = df[df['CALL_TYPE']=='B']\n",
    "\n",
    "# Keep only the rows that do NOT comply with the rule:\n",
    "# ORIGIN_STAND must be numeric, ORIGIN_CALL must be NULL\n",
    "violations_B = rule_B[~(rule_B['ORIGIN_STAND'].apply(is_valid_id) & rule_B['ORIGIN_CALL'].isnull())]\n",
    "\n",
    "# Number of violating rows\n",
    "print(\"Number of violations for CALL_TYPE 'B':\", len(violations_B))\n",
    "\n",
    "# Optionally, inspect the first few violating rows\n",
    "violations_B.head()"
   ],
   "id": "237b6c22a7e050c2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of violations for CALL_TYPE 'B': 11302\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "                 TRIP_ID CALL_TYPE  ORIGIN_CALL  ORIGIN_STAND   TAXI_ID  \\\n",
       "136  1372638560620000308         B          NaN           NaN  20000308   \n",
       "280  1372650696620000320         B          NaN           NaN  20000320   \n",
       "433  1372656129620000307         B          NaN           NaN  20000307   \n",
       "640  1372663170620000391         B          NaN           NaN  20000391   \n",
       "564  1372665032620000004         B          NaN           NaN  20000004   \n",
       "\n",
       "                        TIMESTAMP  MISSING_DATA  \\\n",
       "136 1970-01-01 00:00:01.372638560         False   \n",
       "280 1970-01-01 00:00:01.372650696         False   \n",
       "433 1970-01-01 00:00:01.372656129         False   \n",
       "640 1970-01-01 00:00:01.372663170         False   \n",
       "564 1970-01-01 00:00:01.372665032         False   \n",
       "\n",
       "                                              POLYLINE  \n",
       "136  [[-8.58564,41.148567],[-8.585667,41.148873],[-...  \n",
       "280  [[-8.6058,41.153517],[-8.606133,41.153472],[-8...  \n",
       "433  [[-8.63064,41.154714],[-8.630685,41.154102],[-...  \n",
       "640  [[-8.630712,41.154966],[-8.630775,41.154867],[...  \n",
       "564  [[-8.591319,41.156181],[-8.591463,41.156217],[...  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TRIP_ID</th>\n",
       "      <th>CALL_TYPE</th>\n",
       "      <th>ORIGIN_CALL</th>\n",
       "      <th>ORIGIN_STAND</th>\n",
       "      <th>TAXI_ID</th>\n",
       "      <th>TIMESTAMP</th>\n",
       "      <th>MISSING_DATA</th>\n",
       "      <th>POLYLINE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>1372638560620000308</td>\n",
       "      <td>B</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20000308</td>\n",
       "      <td>1970-01-01 00:00:01.372638560</td>\n",
       "      <td>False</td>\n",
       "      <td>[[-8.58564,41.148567],[-8.585667,41.148873],[-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>280</th>\n",
       "      <td>1372650696620000320</td>\n",
       "      <td>B</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20000320</td>\n",
       "      <td>1970-01-01 00:00:01.372650696</td>\n",
       "      <td>False</td>\n",
       "      <td>[[-8.6058,41.153517],[-8.606133,41.153472],[-8...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>433</th>\n",
       "      <td>1372656129620000307</td>\n",
       "      <td>B</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20000307</td>\n",
       "      <td>1970-01-01 00:00:01.372656129</td>\n",
       "      <td>False</td>\n",
       "      <td>[[-8.63064,41.154714],[-8.630685,41.154102],[-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>640</th>\n",
       "      <td>1372663170620000391</td>\n",
       "      <td>B</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20000391</td>\n",
       "      <td>1970-01-01 00:00:01.372663170</td>\n",
       "      <td>False</td>\n",
       "      <td>[[-8.630712,41.154966],[-8.630775,41.154867],[...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>1372665032620000004</td>\n",
       "      <td>B</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20000004</td>\n",
       "      <td>1970-01-01 00:00:01.372665032</td>\n",
       "      <td>False</td>\n",
       "      <td>[[-8.591319,41.156181],[-8.591463,41.156217],[...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "There are different cases that may cause this problem:\n",
    "- Both CALL_TYPE and ORIGIN_CALL are null. In this case we can change CALL_TYPE to C so that it satisfies the rule\n",
    "- None of the two values are null. In this case we can change ORIGIN_CALL to null so that is satisfies the rule of CALL_TYPE='B'\n",
    "- ORIGIN_STAND is the one with the null value. As this case is the one that satisfies the rule when CALL_TYPE is A, we can change it's value from B to A\n",
    "\n",
    "Get the number of rows of each case"
   ],
   "id": "7ae198fa1461a97b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-07T15:31:15.292418800Z",
     "start_time": "2025-10-07T15:12:39.288779Z"
    }
   },
   "cell_type": "code",
   "source": [
    "mask_B = df['CALL_TYPE'] == 'B'\n",
    "\n",
    "mask_case1 = mask_B & df['ORIGIN_CALL'].isnull() & df['ORIGIN_STAND'].isnull()\n",
    "mask_case2 = mask_B & df['ORIGIN_CALL'].notnull() & df['ORIGIN_STAND'].notnull()\n",
    "mask_case3 = mask_B & df['ORIGIN_CALL'].notnull() & df['ORIGIN_STAND'].isnull()\n",
    "\n",
    "# DataFrames with violating rows (one per case)\n",
    "violations_case1 = df[mask_case1].copy()\n",
    "violations_case2 = df[mask_case2].copy()\n",
    "violations_case3 = df[mask_case3].copy()\n",
    "\n",
    "# Counts\n",
    "total_B = mask_B.sum()\n",
    "count_case1 = mask_case1.sum()\n",
    "count_case2 = mask_case2.sum()\n",
    "count_case3 = mask_case3.sum()\n",
    "count_non_violating_B = total_B - (count_case1 + count_case2 + count_case3)\n",
    "\n",
    "# Print summary\n",
    "print(\"Summary of CALL_TYPE == 'B' rows and violation cases\")\n",
    "print(\"--------------------------------------------------\")\n",
    "print(f\"Total rows with CALL_TYPE == 'B': {total_B}\")\n",
    "print(f\"Case 1 (ORIGIN_CALL NULL & ORIGIN_STAND NULL)  : {count_case1}  ({count_case1/total_B:.2%} of B rows)\")\n",
    "print(f\"Case 2 (ORIGIN_CALL NOT NULL & ORIGIN_STAND NOT NULL): {count_case2}  ({count_case2/total_B:.2%} of B rows)\")\n",
    "print(f\"Case 3 (ORIGIN_CALL NOT NULL & ORIGIN_STAND NULL): {count_case3}  ({count_case3/total_B:.2%} of B rows)\")\n"
   ],
   "id": "19162f84585fb80b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary of CALL_TYPE == 'B' rows and violation cases\n",
      "--------------------------------------------------\n",
      "Total rows with CALL_TYPE == 'B': 817881\n",
      "Case 1 (ORIGIN_CALL NULL & ORIGIN_STAND NULL)  : 11302  (1.38% of B rows)\n",
      "Case 2 (ORIGIN_CALL NOT NULL & ORIGIN_STAND NOT NULL): 0  (0.00% of B rows)\n",
      "Case 3 (ORIGIN_CALL NOT NULL & ORIGIN_STAND NULL): 0  (0.00% of B rows)\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "As every violation of the rule occurs when both ORIGIN_CALL and ORIGIN_STAND were null we only change the rows as it was explained earlier",
   "id": "17d824aada48017b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-07T15:31:15.292418800Z",
     "start_time": "2025-10-07T15:14:15.843273Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Dictionary to keep logs of changes\n",
    "change_log = {'Case': [], 'Index': [], 'Original_CALL_TYPE': [], 'Original_ORIGIN_CALL': [], 'Original_ORIGIN_STAND': [], 'New_CALL_TYPE': [], 'New_ORIGIN_CALL': []}\n",
    "\n",
    "# -----------------------------\n",
    "# Case 1: Both ORIGIN_CALL and ORIGIN_STAND are NULL for CALL_TYPE 'B'\n",
    "mask_case1 = (df['CALL_TYPE'] == 'B') & df['ORIGIN_CALL'].isnull() & df['ORIGIN_STAND'].isnull()\n",
    "for idx in df[mask_case1].index:\n",
    "    change_log['Case'].append('Case 1 (B→C)')\n",
    "    change_log['Index'].append(idx)\n",
    "    change_log['Original_CALL_TYPE'].append(df.at[idx, 'CALL_TYPE'])\n",
    "    change_log['Original_ORIGIN_CALL'].append(df.at[idx, 'ORIGIN_CALL'])\n",
    "    change_log['Original_ORIGIN_STAND'].append(df.at[idx, 'ORIGIN_STAND'])\n",
    "    change_log['New_CALL_TYPE'].append('C')\n",
    "    change_log['New_ORIGIN_CALL'].append(df.at[idx, 'ORIGIN_CALL'])\n",
    "    df.at[idx, 'CALL_TYPE'] = 'C'"
   ],
   "id": "b65f740b72f1be9d",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Copy the cleaned dataframe",
   "id": "bcd877db00d78377"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Save cleaned DataFrame to CSV\n",
    "for col in df.columns:\n",
    "    nulls = df[col].isnull().sum()\n",
    "    unique_vals = df[col].nunique(dropna=True)\n",
    "    repeated = len(df[col]) - unique_vals - nulls  # Total minus unique values and nulls\n",
    "    summary = pd.concat([summary,\n",
    "                         pd.DataFrame([[col, nulls, unique_vals, repeated]],\n",
    "                                             columns=['Column', 'Nulls', 'Unique Values', 'Repeated Values'])],\n",
    "                        ignore_index=True)\n",
    "\n",
    "# Show summary\n",
    "print(summary)\n",
    "\n",
    "mask_B = df['CALL_TYPE'] == 'B'\n",
    "\n",
    "mask_case1 = mask_B & df['ORIGIN_CALL'].isnull() & df['ORIGIN_STAND'].isnull()\n",
    "mask_case2 = mask_B & df['ORIGIN_CALL'].notnull() & df['ORIGIN_STAND'].notnull()\n",
    "mask_case3 = mask_B & df['ORIGIN_CALL'].notnull() & df['ORIGIN_STAND'].isnull()\n",
    "\n",
    "# DataFrames with violating rows (one per case)\n",
    "violations_case1 = df[mask_case1].copy()\n",
    "violations_case2 = df[mask_case2].copy()\n",
    "violations_case3 = df[mask_case3].copy()\n",
    "\n",
    "# Counts\n",
    "total_B = mask_B.sum()\n",
    "count_case1 = mask_case1.sum()\n",
    "count_case2 = mask_case2.sum()\n",
    "count_case3 = mask_case3.sum()\n",
    "count_non_violating_B = total_B - (count_case1 + count_case2 + count_case3)\n",
    "\n",
    "# Print summary\n",
    "print(\"Summary of CALL_TYPE == 'B' rows and violation cases\")\n",
    "print(\"--------------------------------------------------\")\n",
    "print(f\"Total rows with CALL_TYPE == 'B': {total_B}\")\n",
    "print(f\"Case 1 (ORIGIN_CALL NULL & ORIGIN_STAND NULL)  : {count_case1}  ({count_case1/total_B:.2%} of B rows)\")\n",
    "print(f\"Case 2 (ORIGIN_CALL NOT NULL & ORIGIN_STAND NOT NULL): {count_case2}  ({count_case2/total_B:.2%} of B rows)\")\n",
    "print(f\"Case 3 (ORIGIN_CALL NOT NULL & ORIGIN_STAND NULL): {count_case3}  ({count_case3/total_B:.2%} of B rows)\")"
   ],
   "id": "b6ee44755820cc47"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Change datatype of ORIGIN_CALL and ORIGIN_STAND from float to int",
   "id": "a00e2f023d66f5df"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "df[\"ORIGIN_CALL\"] = df[\"ORIGIN_CALL\"].astype(\"Int64\")\n",
    "df[\"ORIGIN_STAND\"] = df[\"ORIGIN_STAND\"].astype(\"Int64\")"
   ],
   "id": "a53fbe1d73f6a063"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Check the max number of decimal in the dataset",
   "id": "71949352e6e42c71"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import ast\n",
    "\n",
    "def count_decimals(num):\n",
    "    \"\"\"Return the number of decimals in a float, ignoring scientific notation.\"\"\"\n",
    "    s = str(num)\n",
    "    if '.' in s:\n",
    "        return len(s.split('.')[-1].rstrip('0'))  # remove trailing zeros\n",
    "    return 0\n",
    "\n",
    "def max_decimals_in_polyline(polyline_str):\n",
    "    \"\"\"Given a POLYLINE string (list of [lon, lat]), return max decimals in any point.\"\"\"\n",
    "    if not polyline_str or polyline_str == '[]':\n",
    "        return 0\n",
    "    try:\n",
    "        coords = ast.literal_eval(polyline_str)\n",
    "        return max(\n",
    "            count_decimals(coord)\n",
    "            for point in coords\n",
    "            for coord in point\n",
    "        )\n",
    "    except Exception:\n",
    "        return 0\n",
    "\n",
    "# Apply function to all rows and find global max\n",
    "max_decimals = df['POLYLINE'].apply(max_decimals_in_polyline).max()\n",
    "\n",
    "print(f\"Maximum number of decimal places in POLYLINE: {max_decimals}\")\n",
    "\n",
    "df.to_csv(\"../dat/clean.csv\", index=False)"
   ],
   "id": "5e98ac3cbf7949f5"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
